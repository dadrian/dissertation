% !TEX root ../proposal.tex

Cryptography is a field born from mathematics, and much of cryptographic
research is concerned with proving correctness of both of primitives and
protocols, given some set of assumptions. Cryptography is one of the only
components of security that can be provably secure. As a result, the
cryptography is often considered to be one of the strongest components of
security.

Yet historically, cryptography has been one of the most fragile aspects of
security. This is not because of issues with the underlying math or the
underlying primitives being insecure, but is instead security that is lost in
translation between the cryptographers and the implementers.

The process of going from paper to program, or from proof-of-concept to
production, introduces mistakes and misunderstandings, which leads to
cryptographic failures and insecurity. To address this, the cryptographic
research community is beginning to introduce and study new concepts such as
misuse-resistant cryptography, and simplified cryptographic APIs.
Unfortunately, the state of the art in cryptography engineering remains
considerably behind the state of art in research.

To understand where mistakes are being made, and understand where complexity
arises from, we must understand how cryptography is actually being used. To
better use cryptography to secure the Internet, we must first under how
cryptography is being used. Examining CVE databases and trawling through
cryptographic code provides one lens into real-world deployments of
cryptography, but it does not necessarily reflect the state of the Internet.

Large-scale empirical methods allow us to characterize how cryptography is
being used on the Internet, today. Although cryptography is a formal and
mathematical science, empirical methods provide additional insight into the
fundamental nature of the use of cryptography to secure communication. We have
already used empirical methods to make a tangible and measurable impact on the
security of the Internet today. We can further use the insights gained from
empirical methods to better secure the Internet in the future. 

Applying empirical methods to cryptography requires solving engineering
challenges. A fundamental methodology underlying the empirical study of
cryptography is Internet-wide scanning. While tooling such as ZMap drastically
reduced the barrier to entry, Internet-wide scanning still requires additional
application-layer tooling to be useful to study cryptography (e.g. the ability
to perform large numbers of TLS handshakes), as well as careful experimental
design to extract the necessary information from every web server on the
Internet, in a systematic, repeatable and consistent method. Data collected
from a single scan often approaches one terabyte in size, and correlating
across multiple scans may involve processing billions of application-layer
handshakes.

In this dissertation I show contributions made to the measurement field itself,
and show three different cases where empiricism enabled additional insight when
combined with other methods cryptography research.

\section{Faster Internet-Wide Scanning}

I first discuss contributions to the measurement field, which underlies
cryptographic empiricisim. There is an abundance of tooling for both active and
passive network measurement. It is tempting to assume that the tools required
to study cryptography at Internet-scale already exist, however this is often
not the case. If the tooling does exist, it often requires herculean effort to
answer simple questions, such as ``how many HTTPS hosts still support 512-bit
key Diffie-Hellman key exchange?''. While often overlooked, understanding,
contributing, and building tooling is a key aspect of participating in
empirical cryptographic research.

As a single example of contributions in this space, in \S\ref{chapter:zippier},
I show improvements to the ZMap scanner which enable it to operate at a full
10Gbps line rate~\cite{zippier-zmap-2014}.  While ZMap enabled the use of
Internet-wide scanning accessible as a measurement method, this work moves
towards enabling hourly or real-time measurement of the cryptographic behavior
of all Internet-connected systems.

When originally introduced, ZMap was capable of saturating a 1Gbps uplink from
a single host, enabling an Internet-wide TCP SYN scan of IPv4 to be performed
in forty-five minutes. However, when used with a 10Gbps network interface, ZMap
reached barely above the 1Gbps mark. The required thread synchronization during
address generation restricted the performance benefit of threading, and limited
the ability to leverage multi-core systems. Furthermore, the copy from user
space to kernel memory when sending a packet limited total throughput. Scanning
a 10Gbps requires sending nearly 15 million packets per second continuously,
which allows for only 200 cycles per packet on a 3 GHz system.

I introduced performance improvements to address both of these constraints, and
enable ZMap to fully utilize a 10Gbps network link, bringing the total time for
a TCP SYN scan of IPv4 to under five minutes from a single host. While
Internet-measurement is often used to provide coarse-grain understanding of the
shape of the Internet as a whole, improvements in measurement-collection begin
to move the field towards being able to continuously understand the behavior of
individual hosts, but at global scale.

\section{Cost-Effective Global Attacks}

Discrete-log and factoring based attacks are generally considered out of reach
of attackers.  However, informing such attacks with measurement data allows for
cost-effective attacks that leverage a single, expensive pre-computation to
cheaply attack TLS connections. Furthermore, when combined with broad support
for cryptography that was assumed to be “dead”, these attacks become even
cheaper.



\paragraph{Export Cryptography}
Cryptography has been a regulated as part of the International Traffic in Arms
Regulations (ITAR), for decades. In the 1990s, the Department of State chose to
implement these regulations by limiting any ``exported'' cryptography to
40-bits of security for symmetric ciphers, and 512-bits for security for
public-key cryptography. Authentication strength (\eg MAC length), was not
regulated. After several court cases started in 1995 surrounding Daniel J.
Bernstein's ``Snuffle'' cryptosystem, the regulations were weakened and moved
from the Department of State to the Department of Commerce. The litigation
ended in 1999. However, during this time \ssltwo was designed and deprecated,
SSLv3 was created and then renamed to TLS and moved into the purview of the
IETF, which standardized TLSv1.0 in 1999. All three of these protocols
contained compliance mechanisms for ``export cryptography''.

\subsection{Attacks on Diffie-Hellman}

Diffie-Hellman key exchange is one of the most common public-key
cryptographic methods in use in the Internet. In finite field Diffie-Hellman,
Alice and Bob agree on a large prime $p$ and an integer $g$ modulo $p$. Alice
chooses a secret integer $x_a$ and transmits a public value $g^{x_a} \bmod
p$; Bob chooses a secret integer $x_b$ and transmits his public value
$g^{x_b} \bmod p$. Both Alice and Bob can reconstruct a shared secret $g^{x_a
x_b} \bmod p$, but the best known way for a passive eavesdropper to
reconstruct this secret is to compute the discrete log of either Alice or
Bob's public value. Specifically, given $g$, $p$, and $g^x \bmod p$, an
attacker must calculate $x$.

We uncovered several weaknesses in how finite-field Diffie-Hellman key
exchange was deployed, which drastically affected the cost of decrypting
large amounts of TLS traffic. The algorithmic complexity of calculating
discrete log is exponential, but the bulk of this computation is dependent
solely on $p$, not the individual secrets $a$ and $b$ chosen by each party.
If many hosts use the same groups, then the precomputation cost may be
amortized across all the connections across these hosts, rather than
requiring core-centuries per observed key exchange. This raise an obvious
empirical question: do many hosts share the same set of Diffie-Hellman
parameters, and what is the strength of the parameters?

The TLS protocol contains ``export-grade'' Diffie-Hellman ciphers which use
short 512-bit groups. We show that there is a protocol vulnerability in TLS,
named Logjam, which allows an attacker who can calculate 512-bit discrete logs
to downgrade connections to export-grade Diffie-Hellman ciphers, and decrypt
them. If a TLS session were to use 512-bit Diffie-Hellman, it may first appear
that individual connections could be broken in 60,000 core-hours, or 120 hours
in parallel on commodity hardware. While this is certainly insecure, at first
glance it would appear these connections have a small amount forward secrecy,
by virtue of using ephemeral Diffie-Hellman key, e.g. each connection would
require another 120 hours to be decrypted. This slow process would prohibit
active attacks and limit the risk to passive decryption after the fact. This
again falls prey to precomputation: if many hosts were to support the same weak
parameters, then the computation could be amortized, and individual connections
could be broken in real-time, enabling active attacks. In fact, shared sets of
parameters is what enables the downgrade. In this case, empirical measurement
showed that 80\% of vulnerable hosts used the same set of parameters, moving
this attack from the theoretical to the practical. While recently there has
been a trend towards elliptic curve cryptography, prime-field based
Diffie-Hellman remained common in TLS until 2016, when both Firefox and Chrome
removed it from their default cipher suites as a result of our work.

\subsection{Cross-Protocol Attacks}

Reusing TLS keys across multiple protocols, such as HTTPS, SMTP, and IMAP,
leads to an increased attack surface. Empirical methods allow us to understand
the attack surface increase from key reuse. Furthermore, specific
vulnerabilities in the TLS protocol and older implementations can be utilized
in a cross-protocol context to attack users of a web service without explicitly
compromising the private key. This is best shown by the DROWN vulnerability, in
which the mere existence of an \ssltwo host that shared a key with a TLS host
enabled decryption of otherwise secure TLS connections using modern
cryptography.

The DROWN attack further exploited export-grade cryptography with an additional
novel insight: Bleichenbacher oracles need not be present in the target
protocol under attack, so long as the key is shared between the two protocols.
Specifically, DROWN shows how to use protocol vulnerabilities in \ssltwo to
attack TLS 1.2. The \ssltwo protocol includes support export symmetric ciphers
which are seeded via only five bytes of key material encrypted using RSA \PKCS.
The \ssltwo protocol also requires the server to send data to the client that is
derived from the shared secret, without first verifying that the client has
possession of the secret. When combined with the malleability of RSA,
culminates in a Bleichenbacher oracle that can be used to attack TLS 1.2

Beyond DROWN, the TLS protocol has a fundamentally cross-protocol attack
surface. X.509 certificates are not bound to any particular protocol or port.
Furthermore, even if distinct services, such as mail and web servers, use
different keys, so long as they share any name on the certificate, the
transport-layer security of all connections to that name are limited to the
security of the weakest TLS implementation or configuration. Even traditionally
web-based padding oracle attacks, such as POODLE, or the AES-NI padding-oracle
in OpenSSL, non-web servers can be exploited by active attackers targeting web
users. The attacker can rewrite the TCP connection to an alternative port, and
fill-in any pre-handshake protocol dialogue (e.g. by sending an EHLO or
STARTTLS command in SMTP). Ignoring vulnerabilities in TLS itself, an unpatched
piece of software with a known RCE using the same key as a well-configured and
up to date web server places web clients, should the key be stolen via
traditional software exploitation. We can place an upper bound on the increased
attack surface, by measuring key and name reuse across TLS in different
application-layer protocols on different ports.

\subsection{Weaknesses from Export Cryptography}

As shown by Freak, Logjam, and DROWN, the security of TLS and export
cryptography are fundamentally linked. Export cryptography is a unique
constraint with a fundamentally dangerous goal: weaken cryptography, without
weakening cryptography. Internet measurement techniques show us that the export
regulations weakened protocol design to the point where the regulations are
directly harmful to the security of the Internet today. These empirical
techniques show that these attacks are not theoretical, leveraging protocols
that have long-since disappeared, but instead are a dark side of backwards
compatibility, harming real users today. Although the regulations went out of
effect by 1999, the cryptography remains. At their respective times of
disclosure, 36.7\% of IPv4 HTTPS hosts were vulnerable to FREAK, 4.9\% were
vulnerable to Logjam, and 26\% were vulnerable to DROWN. All forms of export
cryptography have been broken: export RSA key exchange was broken by FREAK,
export Diffie-Hellman key exchange was broken by Logjam, and export symmetric
ciphers were broken by DROWN. In all cases, empirical research enabled the full
understanding of the effects and impacts of these issues.

\section{Fragile Cryptographic Ecosystems}

Often, cryptographers are aware of attacks for decades, but there is a
knowledge gap between the implementers and researchers. A specific example of
this is prime-field based Diffie-Hellman key exchange, in which
implementation and hosts often use groups which unnecessarily open up the
likelihood of small-subgroup confinement and key recovery
attacks~\cite{subgroup-2017}. I used empirical techniques provide insight
into the Diffie-Hellman group selection at Internet-scale, showing that while
a common recommendation is that $p$ should be a ``safe'' prime such that $p =
2q+1$ for some prime $q$, many implementations instead use non-safe ``DSA''
parameters with potentially unsafe subgroups of order $q$. Several standards,
including NIST~SP~800-56A~\cite{sp800} and RFC~5114~\cite{rfc5114}, advocate
the use of these parameters for Diffie-Hellman key exchange, and while it is
possible to use such parameters securely, additional validation checks are
necessary to prevent small-subgroup attacks.

We measured the prevalence of these parameter choices in the wild for HTTPS,
POP3S, SMTP with STARTTLS, SSH, IKEv1, and IKEv2, finding millions of hosts
using DSA and other non-``safe'' primes for Diffie-Hellman key exchange, many
of them in combination with potentially vulnerable behaviors. Beyond simply
using DSA primes, small subgroup attacks require a number of complex, special
conditions to go wrong in order to be feasible, described in
\S\ref{chapter:subgroup}. While seems unlikely that any implementation would
satisfy enough of these requirements to be vulnerable to an attack, it also
seemed unlikely that implementations would use non-safe primes for key
exchange in the first place. Empirical methods did not reveal an
Internet-wide vulnerability, but rather an Internet-wide case of accidental
complexity and fragility. Given the amount of complexity exposed by the
underlying cryptographic APIs for Diffie-Hellman, it is remarkable that any
implementation was safe. Understanding the root causes of this complexity and
confusion enables better protocol design in the future.


% CONCLUSION THOUGHTS
% 
% ENGINERING?
% 
% Applying empirical measurement techniques accurately requires careful
% application of the scientific method and experiment design. However, the field
% of Internet-measurement has strong engineering underpinnings which are often
% overlooked. ZMap now provides efficient transport-layer scanning, but
% application-layer scanning has not yet reached the same efficiency as ZMap.
% Tooling such as ZGrab helps fill in some of these gaps, but is not yet
% transformative. Similarly, simply studying HTTPS at scale creates unique
% engineering constraints, such as how to batch parse and validate X.509
% certificates.

