% !TEX root = ../proposal.tex

Cryptography is mathematical science. Cryptography research is often formal,
and concerned with proving the correctness of primitives and
protocols, given some set of assumptions. Cryptography is one of the only
components of security that can be provably secure. As such, cryptography is
often considered to be one of the strongest components of security.

Yet historically, cryptography has been one of the most \textit{fragile}
aspects of security. This fragility is often not because the underlying math
was incorrect, or because the fundamental cryptographic primitives were
insecure, but because a single mistake in the implementation, configuration,
or protocol can have catastrophic effects on security without affecting the
functional state of the system, leading to communication channels that appear
securely encrypted, but in fact offer little or no security over plaintext.
Security is lost in translation between the cryptographers and protocol
designers, and between protocol designers and implementers. The process of
going from paper to program, or from proof-of-concept to production,
introduces mistakes and misunderstandings and reveals incorrect assumptions.
This leads to cryptographic failures and insecurity.

%To address this, the cryptographic research community is beginning to
%introduce and study new concepts such as misuse-resistant cryptography, and
%simplified cryptographic APIs. Unfortunately, the state of the art in
%cryptography engineering remains considerably behind the state of art in
%research.

Cryptography is a key component in the security of the Internet.
Unfortunately, the fragile nature of cryptography can be compounded by the
large-scale, distributed, and diverse nature of the Internet. Any protocol
can have many implementations, each of which may implement different subsets
of the protocol, or support different configurations. Many protocols are
designed for ``cryptographic agility'', and so identical implementations may
support different sets of underlying algorithms. Devices that support modern
cryptography may also support decades-old broken cryptography. This leads to
an Internet of fragile cryptographic ecosystems, consisting of diverse sets
of clients and servers, many of which use vulnerable or close-to-vulnerable
cryptography.

%To better use cryptography to secure the Internet, we must first
%understand how cryptography is being used on the Internet. Examining CVE databases
%and trawling through cryptographic code provides one lens into real-world
%deployments of cryptography, but does not necessarily reflect the state of
%the Internet.

To better use cryptography to secure the Internet, we need to understand what
types of cryptography are in use, and where mistakes are being made. We must
understand which cryptographic ecosystems are fragile, and which are
resiliant. To deploy new and stronger cryptography, we need to validate our
assumptions about the current cryptographic behavior of devices on the
Internet, so that we can identify which probems need to be solved, and
prioritize their solutions. Large-scale empirical methods allow us to observe
fragility in how cryptography is being used on the Internet, identify new
vulnerabilities, and better secure the Internet in the future.

In this dissertation, I show how empirical measurements collected using
Internet-wide scanning provides insight into how cryptography is used to
secure the Internet. First, I show contributions made to the field of
Internet measurement field itself via improved Internet-wide scanning.
Second, I measure the Internet's resiliancy to small subgroup attacks in
finite-field Diffie-Hellman. Third, I use empirical methods to show how
1990s-era export-grade encryption harmed the security of the Internet for two
decades.

\section{Techniques for Measuring Internet Cryptography}

Internet-wide scanning is a fundmental technique used for empirical study of
cryptography. Large-scale, horizontal port scanners such as
ZMap~\cite{zmap-2013} drastically reduce the barrier to entry to leveraging
network scan data at Internet scale, however port scanning alone is not a
complete solution. Using Internet-wide scanning to measure cryptography is a
three step process:
\begin{enumerate}
  \item \textbf{Identify Hosts.}
    There are $\sim$4B possible IPv4 addresses, however only a subset are
    listening on any given L4 port. A single scan by ZMap or another
    large-scale, asynchronous L3/L4 scanner identifies which hosts have an
    input port open. For example, only $\sim$50M hosts respond on TCP port 443
    (HTTPS).
  \item \textbf{Measure Hosts.}
    ZMap provides no application-layer information about a host. Furthermore,
    hosts that respond on a standard port for some application-layer protocol
    might not actually be configured to speak that protocol. For example,
    only $\sim$38M of the $\sim$50M hosts with TCP port 443 open are TLS
    servers. To collect cryptographic data, an application-layer scanner such
    as ZGrab~\cite{zgrab-github} must connect to all hosts found by ZMap and
    attempt a protocol handshake that records the cryptographic state used
    for the connection.
  \item \textbf{Answer Questions.}
    As is the case in any empirical science, the measurement data is analyzed
    to answer questions such as ``What percentage of TLS servers support
    512-bit Diffie-Hellman?''. Tools such as Censys~\cite{censys} allow
    researchers to ask questions about Internet-wide scan data much faster
    and easier than by trawling through results from individual scans, each
    of which can generate terabytes of data.
\end{enumerate}

With current technology, Internet-wide scanning is useful for creating an
aggregate understanding of the Internet over time. However, it is not yet
able to provide a global understanding of \textit{individual} hosts and
networks. In \S\ref{chapter:zippier}, I discuss contributions to
Internet-wide scanning that provide a foundation to build a more accurate and
complete picture of the Internet and I show improvements to the ZMap scanner
which enable it to operate at a full 10Gbps line
rate~\cite{zippier-zmap-2014}. While ZMap enabled the use of Internet-wide
scanning accessible as a measurement method, this work moves towards enabling
hourly or real-time measurement of the cryptographic behavior of all
Internet-connected systems.

When originally introduced, ZMap was capable of saturating a 1Gbps uplink from
a single host, enabling an Internet-wide TCP SYN scan of IPv4 to be performed
in forty-five minutes. However, when used with a 10Gbps network interface, ZMap
reached barely above the 1Gbps mark. The required thread synchronization during
address generation restricted the performance benefit of threading, and limited
the ability to leverage multi-core systems. Furthermore, the copy from user
space to kernel memory when sending a packet limited total throughput.
Scanning at 10Gbps requires sending nearly 15 million packets per second
continuously, which allows for only 200 cycles per packet on a 3 GHz system.

I introduced performance improvements to address both of these constraints, and
enable ZMap to fully utilize a 10Gbps network link, bringing the total time for
a TCP SYN scan of IPv4 to under five minutes from a single host. While
Internet-measurement is often used to provide coarse-grain understanding of the
shape of the Internet as a whole, improvements in measurement-collection begin
to move the field towards being able to continuously understand the behavior of
individual hosts, but at global scale.

\section{Measuring Diffie-Hellman}

Diffie-Hellman key exchange is one of the most common public-key
cryptographic methods in use in the Internet. In finite field Diffie-Hellman,
Alice and Bob agree on a large prime $p$ and an integer $g$ modulo $p$. Alice
chooses a secret integer $x_a$ and transmits a public value $g^{x_a} \bmod
p$; Bob chooses a secret integer $x_b$ and transmits his public value
$g^{x_b} \bmod p$. Both Alice and Bob can reconstruct a shared secret $g^{x_a
x_b} \bmod p$, but the best known way for a passive eavesdropper to
reconstruct this secret is to compute the discrete log of either Alice or
Bob's public value. Specifically, given $g$, $p$, and $g^x \bmod p$, an
attacker must calculate $x$.

Both $g$ and $p$ must be carefully selected to ensure that the discrete
logarithm problem remains hard, and that the key exchange is not vulnerable
to small subgroup attacks. I used empirical techniques to provide insight
into the Diffie-Hellman group selection at Internet-scale, showing that while
a common recommendation is that $p$ should be a ``safe'' prime such that $p =
2q+1$ for some prime $q$, many implementations instead use non-safe ``DSA''
parameters with potentially unsafe subgroups of order
$q$~\cite{subgroup-2017}. Several standards, including
NIST~SP~800-56A~\cite{sp800} and RFC~5114~\cite{rfc5114}, advocate the use of
these parameters for Diffie-Hellman key exchange, and while it is possible to
use such parameters securely, additional validation checks are necessary to
prevent small-subgroup attacks. This is further evidence of a knowledge gap
between researchers, protocol designers, and implementers. Cryptographers
have been aware of small subgroup attacks for two decades~\cite{lim-1997},
but this has not translated into reliable and consistent defenses being built
into protocols and implementations, likely due to the complexity of the
trade-offs surrounding finite-field parameter selection.

In \S\ref{chapter:subgroup}, I discuss how we measured vulnerability to small
subgroup attacks in the wild for HTTPS, POP3S, SMTP with STARTTLS, SSH,
IKEv1, and IKEv2, finding millions of hosts using DSA and other non-``safe''
primes for Diffie-Hellman key exchange, many of them in combination with
potentially vulnerable behaviors. Beyond simply using DSA primes, small
subgroup attacks require a number of complex, special conditions to go wrong
in order to be feasible. While it seems unlikely that any implementation
would satisfy enough of these requirements to be vulnerable to an attack, it
also seemed unlikely that implementations would use non-safe primes for key
exchange in the first place. Empirical methods did not reveal an
Internet-wide vulnerability, but rather an Internet-wide case of accidental
complexity and fragility. Given the amount of complexity exposed by the
underlying cryptographic APIs for Diffie-Hellman, it is remarkable that any
implementation was safe. Understanding the root causes of this complexity and
confusion, and understanding how it manifests on the Internet, enables better
protocol design in the future.

\section{Measuring Export Cryptography}

The Department of State regulated cryptography in the United States during
the 1990s. Cryptography was covered by the Internation Traffic in Arms
Regulations (ITAR), which broadly limited the ability for US persons to
``export'' cryptography. Beginning in 1995, these regulations were challenged
in court by Daniel J. Bernstein, who was attempting to publish the
``Snuffle'' cryptosystem. The regulations were moved in 1996 from ITAR to the
Export Administration Regulations (EAR), under the control of the Department
of Commerce~\cite{djb-case-status}. Under EAR, ``exported'' cryptography was
limited to 40-bits of security for symmetric ciphers, and 512-bits for
security for public-key cryptography. Authentication strength (\eg MAC
length), was not regulated~\cite{ear-2001-cat-5}. Despite these limitations,
there were major advances in secure channel protocol development during this time.
\ssltwo was designed and deprecated~\cite{sslv2} in 1995. SSLv3 was created and
standardized~\cite{rfc6101} by 1998, and subsequently renamed to TLS and
moved into the purview of the IETF in 1999~\cite{rfc2246}. All three of these
protocol versions contained compliance mechanisms for the export regulations,
where the protocol was capable of negotiating the use of short
``export-grade'' parameters instead of ``modern'' cryptography.

Following further litigation in \textit{Bernstein v. United States}, the key
length limitations from the export regulations were removed in 1999, and
future versions of TLS deprecated the process for using ``export-grade''
cryptography. In \S\ref{chapter:logjam} and \S\ref{chapter:drown}, I discuss
how measuring support for obsolete cryptography that was designed to comply
with these export regulations led to the discovery of vulnerabilities in
modern web browsers.

\subsection{Attacks on RSA}

In early verions of TLS, export compliance when using RSA key exchange was
implemented by limiting the length of the server-provided RSA key to
512-bits. In TLS, the RSA key used for key exchange is usually extracted from
the X.509 certificate used to authenticate the server. However, this key is
also used for non-export cryptography, and so is longer than the max 512-bits
required to comply with the export regulations. To indicate an
export-complaint connection, TLS introduced the \rsaexp{} ciphers. These
ciphers indicate that the server must provide an additional ``export-grade''
RSA key to be used for key exchange. In 2015, Beurdouche et al.\ showed that
many TLS implementations incorrectly accepted a server-provided \rsaexp{} key
on non-export \rsa{} ciphers~\cite{freak-attack-2015}, leading to a downgrade
attack against servers that support \rsaexp{}, denoted FREAK.

We modified the ZMap~\cite{zmap-2013} and ZGrab~\cite{zgrab-github} toolchain
to be able to scan for HTTPS servers that support export RSA ciphers. To
identify these servers, we implemented export RSA key exchange in ZGrab. We
then sent a \textsf{ClientHello} containing only \rsaexp{} ciphers to
identify servers capable of completing the handshake. At the time of
disclosure on March 3, 2015, we found that 36.7\% of HTTPS servers with
browser-trusted certificates supported \rsaexp{} ciphers. This was
unexpected, since no modern web client has offered export-grade ciphers in
its default configuration for over a decade. Prior to measuring support for
export-grade RSA, the FREAK attack seemed far less impactful, since
conventional wisdom was that no server supported export-grade cryptography.
Interestingly, only 26.3\% of all (trusted and untrusted) HTTPS servers were
vulnerable overall, meaning FREAK was more common among trusted hosts than
untrusted hosts. FREAK saw a fast patch rate, dropping to only 6.5\% among
trusted servers, and 11.8\% among untrusted servers on March 10, 2015, one
week after disclosure. In May 2016, vulnerability to FREAK among hosts with
trusted certificates was only 1.68\%.

We instrumented \url{freakattack.com} with a browser-check tool that tested
users browsers for vulnerability to FREAK. To do so, we first implemented a
variant of the attack consisting of a single test server that would
negotiate a non-export RSA cipher, but send an export RSA \textsf{ServerKeyExchange}
message. We created this implementation by extending our modifications to
ZGrab. We added Javascript to the FREAK website that attempted to connect to
a subdomain running the test server, and logged if the connection was
successfully created. Between March 4 and March 7, 2015, we logged 1,260,951
test connections, of which 223,481 (17.7\%) were vulnerable to FREAK. This
client population is limited to people who visited the FREAK disclosure
website, and is therefore likely biased towards the technology and security
community, and is not representable of all Internet users. However, these
results did show that even though Mozilla NSS was not vulnerable to FREAK, of
the 223,481 vulnerable clients, 15,591 (7.0\%) identified as Firefox based on
user-agent strings. This is likely due to anti-virus or other endpoint
security software that acts as a man-in-the-middle for all TLS traffic being
built against a vulnerable TLS library. We experimentally confirmed with
packet traces that enabling Avast Free Antivirus caused Firefox to become
vulnerable to FREAK. Later work by Durumeric~et~al showed systemic
vulnerability to patched TLS attacks among various client-side TLS
interception products~\cite{tls-interception-2017}. This phenomenon was
further observed when attempting to distrust certificates signed with SHA-1.
When Firefox stopped trusted certificates signed using SHA-1 issued more
recently than Dec 31, 2015, many users behind client-side TLS interception
software were unable to access the Internet, as the interception software was
issuing SHA-1 certificates locally to be able to inspect TLS
connections~\cite{firefox-undo-sha1}.


\subsection{Attacks on Diffie-Hellman}

The TLS protocol contains export-grade Diffie-Hellman ciphers, denoted
\dheexp{}, which use short 512-bit groups~\cite{rfc2246}. In
\S\ref{chapter:logjam}, I discuss Logjam, a protocol vulnerability in TLS,
which allows an attacker who can calculate 512-bit discrete logs to downgrade
connections to export-grade Diffie-Hellman ciphers, and decrypt them. Similar
to the FREAK attack~\cite{freak-attack-2015}, the Logjam attack is only
possible when the server supports \dheexp{} ciphers. We used Internet-wide
scanning to determine support for these ciphers.

An individual TLS session using 512-bit Diffie-Hellman could be broken in
60,000 core-hours, or 120 hours in parallel on commodity hardware simply by
calculating a discrete log~\cite{logjam-2015}. While this is certainly
insecure, at first glance it appears these connections have a small amount
forward secrecy, by virtue of using ephemeral Diffie-Hellman key, e.g. each
connection would require another 120 hours to be decrypted. This slow process
would prohibit active attacks and limit the risk to passive decryption after
the fact.

In practice, while the algorithmic complexity of calculating discrete log is
exponential, the bulk of this computation is dependent solely on $p$, not
the individual secrets $x_a$ and $x_b$ chosen by each party. If many hosts use
the same groups, then the precomputation cost may be amortized across all the
connections across these hosts, rather than requiring core-centuries per
observed key exchange. This raise an obvious empirical question: do many
hosts share the same set of Diffie-Hellman parameters, and what is the
strength of the parameters? If many hosts were to support the same weak
parameters, then the computation could be amortized, and individual
connections could be broken in real-time, enabling active attacks. In fact,
shared sets of parameters is what enables the downgrade. In this case,
empirical measurement showed that 80\% of vulnerable hosts used the same set
of parameters, moving the Logjam attack from the theoretical towards the
practical. While there is a recent trend towards elliptic curve cryptography,
prime-field based Diffie-Hellman remained common in TLS until 2016, when both
Firefox and Chrome removed it from their default cipher suites as a result of
our work.

\subsection{Attacks on Symmetric Cryptography}

For export compliance in \ssltwo, rather than weaken key exchange, the
protocol reduces the key length used for the symmetric encryption.
\ssltwo contains support for export symmetric ciphers, which are seeded via
only five bytes of key material encrypted using RSA \PKCS. In
\S\ref{chapter:drown}, I introduce the DROWN attack, which exploits protocol
vulnerabilities in \ssltwo surrounding export-grade cryptography to attack
TLS~\cite{drown-2016}. To be vulnerable to DROWN, a TLS server must share a
key with an SSLv2 server, or share a name with an SSLv2 server that has not
patched the ``extra-clear'' vulnerablity~\cite{drown-2016,cve-2016-0701}.
The \ssltwo protocol requires the server to send data to the client that is
derived from the shared secret, without first verifying that the client has
possession of the secret. When combined with the malleability of RSA, and a
shortened five byte secret, this culminates in a Bleichenbacher oracle that
can be used to attack TLS 1.2. The \ssltwo oracle need not be on the same
host, or even the same protocol as the target---a mail server that supports
\starttls can be used to attack an HTTPS server. I further discuss
measurements for \ssltwo support among both web and non-web protocols, and
characterize the amplified DROWN attack surface that stems from the
cross-protocol nature of the attack.

\section{Empirical Cryptography}

Large-scale empirical methods allow us to observe fragility in how
cryptography is being used on the Internet, identify new vulnerabilities, and
better secure the Internet in the future. In this disseration, I show how
empirical measurements collected using Internet-wide scanning provide insight
into how cryptography is used to secure the Internet. I first show in
Chapter~\ref{chapter:zippier} how Internet-wide scanning and related network
measurement technology can be used to measure cryptography, and where we can
improve it to measure cryptography better. I use Internet-wide scanning to
measure Diffie-Hellman in Chapter~\ref{chapter:subgroup}. To show how
empirical techniques can lead to the discovery of new vulnerabilities, I
discuss in Chapter~\ref{chapter:logjam} and Chapter~\ref{chapter:drown} how
measuring cryptography designed to comply with 1990s-era export regulations
led to the discovery of new attacks against modern clients, and how
empiricism drove our understanding of the impact of these attacks. Finally, I look
forward in Chapter~\ref{chapter:conclusion}, and discuss some of the impact from
empirical cryptography on protocol design and the security of the Internet, and suggest
where else empirical methods can be used for cryptography research.

% CONCLUSION THOUGHTS
%
% ENGINERING?
%
% Applying empirical measurement techniques accurately requires careful
% application of the scientific method and experiment design. However, the field
% of Internet-measurement has strong engineering underpinnings which are often
% overlooked. ZMap now provides efficient transport-layer scanning, but
% application-layer scanning has not yet reached the same efficiency as ZMap.
% Tooling such as ZGrab helps fill in some of these gaps, but is not yet
% transformative. Similarly, simply studying HTTPS at scale creates unique
% engineering constraints, such as how to batch parse and validate X.509
% certificates.

