% !TEX root = ../proposal.tex

Cryptography is mathematical science. Cryptography research is often formal,
and concerned with proving the correctness of primitives and
protocols, given some set of assumptions. Cryptography is one of the only
components of security that can be provably secure. As such, cryptography is
often considered to be one of the strongest components of security.

Yet historically, cryptography has been one of the most \textit{fragile}
aspects of security. This fragility is often not because the underlying math
was incorrect, or because the fundamental cryptographic primitives were
insecure, but because a single mistake in the implementation, configuration,
or protocol can have catastrophic effects on security. Security is lost in
translation between the cryptographers and protocol designers, and between
protocol designers and implementers. The process of going from paper to
program, or from proof-of-concept to production, introduces mistakes and
misunderstandings and reveals incorrect assumptions. This leads to
cryptographic failures and insecurity.

%To address this, the cryptographic research community is beginning to
%introduce and study new concepts such as misuse-resistant cryptography, and
%simplified cryptographic APIs. Unfortunately, the state of the art in
%cryptography engineering remains considerably behind the state of art in
%research.

Cryptography is a key component in the security of the Internet.
Unfortunately, the fragile nature of cryptography can be compounded by the
large-scale, distributed, and diverse nature of the Internet. Any protocol
can have many implementations, each of which may implement different subsets
of the protocol, or support different configurations. Many protocols are
designed for ``cryptographic agility'', and so identical implementations may
support different sets of underlying algorithms. Devices that support modern
cryptography may also support decades-old broken cryptography. This leads to
an Internet of fragile cryptographic ecosystems, consisting of diverse sets
of clients and servers \todo{define this more}.

%To better use cryptography to secure the Internet, we must first
%understand how cryptography is being used on the Internet. Examining CVE databases
%and trawling through cryptographic code provides one lens into real-world
%deployments of cryptography, but does not necessarily reflect the state of
%the Internet.

To better use cryptography to secure the Internet, we need to understand what
types of cryptography are being used, and where mistakes are being made. We
must understand which cryptographic ecosystems are fragile, and which are
resiliant. Large-scale empirical methods allow us to observe fragility in
how cryptography is being used on the Internet, identify new vulnerabilites,
and better secure the Internet in the future.

In this dissertation, I show how empirical measurements collected using
Internet-wide scanning provides insight into how cryptography is used to
secure the Internet. First, I show contributions made to the field of
Internet measurement field itself via improved Internet-wide scanning.
Second, I measure the Internet's resiliancy to small subgroup attacks in
finite-field Diffie-Hellman. Third, I use empirical methods to show how
1990s-era export-grade encryption harmed the security of the Internet for two
decades.

\section{Techniques for Measuring Internet Cryptography}

Internet-wide scanning is a fundmental technique used for empirical study of
cryptography. Large-scale, horizontal port scanners such as
ZMap~\cite{zmap-2013} drastically reduce the barrier to entry to leveraging
network scan data at Internet scale, however port scanning alone is not a
complete solution. Using Internet-wide scanning to measure cryptography is a
three step process:
\begin{enumerate}
  \item \textbf{Identify Hosts.}
    There are $\sim$4B possible IPv4 addresses, however only a subset are
    listening on any given L4 port. A single scan by ZMap or another
    large-scale, asynchronous L3/L4 scanner identifies which hosts have an
    input port open. For example, only $\sim$50M hosts respond on TCP port 443
    (HTTPS).
  \item \textbf{Measure Hosts.}
    ZMap provides no application-layer information about a host. Furthermore,
    hosts that respond on a standard port for some application-layer protocol
    might not actually be configured to speak that protocol. For example,
    only $\sim$38M of the $\sim$50M hosts with TCP port 443 open are TLS
    servers. To collect cryptographic data, an application-layer scanner such
    as ZGrab~\cite{zgrab-github} must connect to all hosts found by ZMap and
    attempt a protocol handshake that records the cryptographic state used
    for the connection.
  \item \textbf{Answer Questions.}
    As is the case in any empirical science, the measurement data is analyzed
    to answer questions such as ``What percentage of TLS servers support
    512-bit Diffie-Hellman?''. Tools such as Censys~\cite{censys} allow
    researchers to ask questions about Internet-wide scan data much faster
    and easier than by trawling through results from individual scans, each
    of which can generate terabytes of data.
\end{enumerate}

With current technology, Internet-wide scanning is useful for creating an
aggregate understanding of the Internet over time. However, it is not yet
able to provide a global understanding of \textit{individual} hosts and
networks. I first discuss contributions to Internet-wide scanning that
provide a foundation to build a more accurate and complete picture of the
Internet in \S\ref{chapter:zippier}. I show improvements to the ZMap scanner
which enable it to operate at a full 10Gbps line
rate~\cite{zippier-zmap-2014}. While ZMap enabled the use of Internet-wide
scanning accessible as a measurement method, this work moves towards enabling
hourly or real-time measurement of the cryptographic behavior of all
Internet-connected systems.

When originally introduced, ZMap was capable of saturating a 1Gbps uplink from
a single host, enabling an Internet-wide TCP SYN scan of IPv4 to be performed
in forty-five minutes. However, when used with a 10Gbps network interface, ZMap
reached barely above the 1Gbps mark. The required thread synchronization during
address generation restricted the performance benefit of threading, and limited
the ability to leverage multi-core systems. Furthermore, the copy from user
space to kernel memory when sending a packet limited total throughput. Scanning
a 10Gbps requires sending nearly 15 million packets per second continuously,
which allows for only 200 cycles per packet on a 3 GHz system.

I introduced performance improvements to address both of these constraints, and
enable ZMap to fully utilize a 10Gbps network link, bringing the total time for
a TCP SYN scan of IPv4 to under five minutes from a single host. While
Internet-measurement is often used to provide coarse-grain understanding of the
shape of the Internet as a whole, improvements in measurement-collection begin
to move the field towards being able to continuously understand the behavior of
individual hosts, but at global scale.

\section{Measuring Diffie-Hellman}

Diffie-Hellman key exchange is one of the most common public-key
cryptographic methods in use in the Internet. In finite field Diffie-Hellman,
Alice and Bob agree on a large prime $p$ and an integer $g$ modulo $p$. Alice
chooses a secret integer $x_a$ and transmits a public value $g^{x_a} \bmod
p$; Bob chooses a secret integer $x_b$ and transmits his public value
$g^{x_b} \bmod p$. Both Alice and Bob can reconstruct a shared secret $g^{x_a
x_b} \bmod p$, but the best known way for a passive eavesdropper to
reconstruct this secret is to compute the discrete log of either Alice or
Bob's public value. Specifically, given $g$, $p$, and $g^x \bmod p$, an
attacker must calculate $x$.

Both $g$ and $p$ must be carefully selected to ensure that the discrete
logarithm problem remains hard, and that the key exchange is not vulnerable
to small subgroup attacks. I used empirical techniques to provide insight
into the Diffie-Hellman group selection at Internet-scale, showing that while
a common recommendation is that $p$ should be a ``safe'' prime such that $p =
2q+1$ for some prime $q$, many implementations instead use non-safe ``DSA''
parameters with potentially unsafe subgroups of order
$q$~\cite{subgroup-2017}. Several standards, including
NIST~SP~800-56A~\cite{sp800} and RFC~5114~\cite{rfc5114}, advocate the use of
these parameters for Diffie-Hellman key exchange, and while it is possible to
use such parameters securely, additional validation checks are necessary to
prevent small-subgroup attacks. This is further evidence of a knowledge gap
between researchers, protocol designers, and implementers. Cryptographers
have been aware of small subgroup attacks for two decades~\cite{lim-1997},
but this has not translated into reliable and consistent defenses being built
into protocols and implementations, likely due to the complexity of the
trade-offs surrounding finite-field parameter selection.

In \S\ref{chapter:subgroup}, I discuss how we measured vulnerability to small
subgroup attacks in the wild for HTTPS, POP3S, SMTP with STARTTLS, SSH,
IKEv1, and IKEv2, finding millions of hosts using DSA and other non-``safe''
primes for Diffie-Hellman key exchange, many of them in combination with
potentially vulnerable behaviors. Beyond simply using DSA primes, small
subgroup attacks require a number of complex, special conditions to go wrong
in order to be feasible. While it seems unlikely that any implementation
would satisfy enough of these requirements to be vulnerable to an attack, it
also seemed unlikely that implementations would use non-safe primes for key
exchange in the first place. Empirical methods did not reveal an
Internet-wide vulnerability, but rather an Internet-wide case of accidental
complexity and fragility. Given the amount of complexity exposed by the
underlying cryptographic APIs for Diffie-Hellman, it is remarkable that any
implementation was safe. Understanding the root causes of this complexity and
confusion, and understanding how it manifests on the Internet, enables better
protocol design in the future.

\section{Measuring Export Cryptography}

%Discrete-log and factoring based attacks are generally considered out of reach
%of attackers.  However, informing such attacks with measurement data allows for
%cost-effective attacks that leverage a single, expensive pre-computation to
%cheaply attack TLS connections. Furthermore, when combined with broad support
%for cryptography that was assumed to be “dead”, these attacks become even
%cheaper.

%\paragraph{Export Cryptography}

The Department of State regulated cryptography in the United States during
the 1990s. Cryptography was covered by the Internation Traffic in Arms
Regulations (ITAR), which broadly limited the ability for US persons to
``export'' cryptography. Beginning in 1995, these regulations were challenged
in court by Daniel J. Bernstein, who was attempting to publish the
``Snuffle'' cryptosystem. The regulations were moved in 1996 from ITAR to the
Export Administration Regulations (EAR), under the control of the Department
of Commerce~\cite{djb-case-status}. Under EAR, ``exported'' cryptography was
limited to 40-bits of security for symmetric ciphers, and 512-bits for
security for public-key cryptography. Authentication strength (\eg MAC
length), was not regulated~\cite{ear-2001-cat-5}. Despite these limitations,
there were major advances in secure channel protocol development during this time.
\ssltwo was designed and deprecated~\cite{sslv2} in 1995. SSLv3 was created and
standardized~\cite{rfc6101} by 1998, and subsequently renamed to TLS and
moved into the purview of the IETF in 1999~\cite{rfc2246}. All three of these
protocol versions contained compliance mechanisms for the export regulations,
where the protocol was capable of negotiating the use of short
``export-grade'' parameters instead of ``modern'' cryptography.

Following further litigation in \textit{Bernstein v. United States}, the key
length limitations from the export regulations were removed in 1999, and
future versions of TLS deprecated the process for using ``export-grade''
cryptography.

\subsection{Attacks on RSA}

In early verions of TLS, export compliance when using RSA key exchange was
implemented by limiting the length of the server-provided RSA key to
512-bits. In TLS, the RSA key used for key exchange is usually extracted from
the X.509 certificate used to authenticate the server. However, this key is
also used for non-export cryptography, and so is longer than the max 512-bits
required to comply with the export regulations. To indicate an
export-complaint connection, TLS introduced the \rsaexp{} ciphers. These
ciphers indicate that the server must provide an additional ``export-grade''
RSA key to be used for key exchange.

Beurdouche et al.\ showed that many TLS implementations incorrectly accepted
a server-provided \rsaexp{} key on non-export \rsa{}
ciphers~\cite{freak-attack-2015}, leading to a downgrade attack against
servers that support \rsaexp{}, denoted FREAK. We measured support for
\rsaexp{}, and found that 36.7\% of HTTPS servers with trusted certificates
still supports \rsaexp{} cipers and were vulnerable to FREAK. This was
unexpected, since no modern web client has offered export-grade ciphers in
its default configuration for over a decade. Prior to measuring support for
export-grade RSA, the FREAK attack seemed far less impactful, since
conventional wisdom was that no server supported export-grade cryptography.

\subsection{Attacks on Diffie-Hellman}

We uncovered several weaknesses in how finite-field Diffie-Hellman key
exchange was deployed, which drastically affected the cost of decrypting
large amounts of TLS traffic. The algorithmic complexity of calculating
discrete log is exponential, but the bulk of this computation is dependent
solely on $p$, not the individual secrets $a$ and $b$ chosen by each party.
If many hosts use the same groups, then the precomputation cost may be
amortized across all the connections across these hosts, rather than
requiring core-centuries per observed key exchange. This raise an obvious
empirical question: do many hosts share the same set of Diffie-Hellman
parameters, and what is the strength of the parameters?

The TLS protocol contains ``export-grade'' Diffie-Hellman ciphers which use
short 512-bit groups. We show that there is a protocol vulnerability in TLS,
named Logjam, which allows an attacker who can calculate 512-bit discrete logs
to downgrade connections to export-grade Diffie-Hellman ciphers, and decrypt
them. If a TLS session were to use 512-bit Diffie-Hellman, it may first appear
that individual connections could be broken in 60,000 core-hours, or 120 hours
in parallel on commodity hardware. While this is certainly insecure, at first
glance it would appear these connections have a small amount forward secrecy,
by virtue of using ephemeral Diffie-Hellman key, e.g. each connection would
require another 120 hours to be decrypted. This slow process would prohibit
active attacks and limit the risk to passive decryption after the fact. This
again falls prey to precomputation: if many hosts were to support the same weak
parameters, then the computation could be amortized, and individual connections
could be broken in real-time, enabling active attacks. In fact, shared sets of
parameters is what enables the downgrade. In this case, empirical measurement
showed that 80\% of vulnerable hosts used the same set of parameters, moving
this attack from the theoretical to the practical. While recently there has
been a trend towards elliptic curve cryptography, prime-field based
Diffie-Hellman remained common in TLS until 2016, when both Firefox and Chrome
removed it from their default cipher suites as a result of our work.

\subsection{Cross-Protocol Attacks}

Reusing TLS keys across multiple protocols, such as HTTPS, SMTP, and IMAP,
leads to an increased attack surface. Empirical methods allow us to understand
the attack surface increase from key reuse. Furthermore, specific
vulnerabilities in the TLS protocol and older implementations can be utilized
in a cross-protocol context to attack users of a web service without explicitly
compromising the private key. This is best shown by the DROWN vulnerability, in
which the mere existence of an \ssltwo host that shared a key with a TLS host
enabled decryption of otherwise secure TLS connections using modern
cryptography.

The DROWN attack further exploited export-grade cryptography with an additional
novel insight: Bleichenbacher oracles need not be present in the target
protocol under attack, so long as the key is shared between the two protocols.
Specifically, DROWN shows how to use protocol vulnerabilities in \ssltwo to
attack TLS 1.2. The \ssltwo protocol includes support export symmetric ciphers
which are seeded via only five bytes of key material encrypted using RSA \PKCS.
The \ssltwo protocol also requires the server to send data to the client that is
derived from the shared secret, without first verifying that the client has
possession of the secret. When combined with the malleability of RSA,
culminates in a Bleichenbacher oracle that can be used to attack TLS 1.2

Beyond DROWN, the TLS protocol has a fundamentally cross-protocol attack
surface. X.509 certificates are not bound to any particular protocol or port.
Furthermore, even if distinct services, such as mail and web servers, use
different keys, so long as they share any name on the certificate, the
transport-layer security of all connections to that name are limited to the
security of the weakest TLS implementation or configuration. Even traditionally
web-based padding oracle attacks, such as POODLE, or the AES-NI padding-oracle
in OpenSSL, non-web servers can be exploited by active attackers targeting web
users. The attacker can rewrite the TCP connection to an alternative port, and
fill-in any pre-handshake protocol dialogue (e.g. by sending an EHLO or
STARTTLS command in SMTP). Ignoring vulnerabilities in TLS itself, an unpatched
piece of software with a known RCE using the same key as a well-configured and
up to date web server places web clients, should the key be stolen via
traditional software exploitation. We can place an upper bound on the increased
attack surface, by measuring key and name reuse across TLS in different
application-layer protocols on different ports.

\subsection{Weaknesses from Export Cryptography}

As shown by Freak, Logjam, and DROWN, the security of TLS and export
cryptography are fundamentally linked. Export cryptography is a unique
constraint with a fundamentally dangerous goal: weaken cryptography, without
weakening cryptography. Internet measurement techniques show us that the export
regulations weakened protocol design to the point where the regulations are
directly harmful to the security of the Internet today. These empirical
techniques show that these attacks are not theoretical, leveraging protocols
that have long-since disappeared, but instead are a dark side of backwards
compatibility, harming real users today. Although the regulations went out of
effect by 1999, the cryptography remains. At their respective times of
disclosure, 36.7\% of IPv4 HTTPS hosts were vulnerable to FREAK, 4.9\% were
vulnerable to Logjam, and 26\% were vulnerable to DROWN. All forms of export
cryptography have been broken: export RSA key exchange was broken by FREAK,
export Diffie-Hellman key exchange was broken by Logjam, and export symmetric
ciphers were broken by DROWN. In all cases, empirical research enabled the full
understanding of the effects and impacts of these issues.

% CONCLUSION THOUGHTS
% 
% ENGINERING?
% 
% Applying empirical measurement techniques accurately requires careful
% application of the scientific method and experiment design. However, the field
% of Internet-measurement has strong engineering underpinnings which are often
% overlooked. ZMap now provides efficient transport-layer scanning, but
% application-layer scanning has not yet reached the same efficiency as ZMap.
% Tooling such as ZGrab helps fill in some of these gaps, but is not yet
% transformative. Similarly, simply studying HTTPS at scale creates unique
% engineering constraints, such as how to batch parse and validate X.509
% certificates.

