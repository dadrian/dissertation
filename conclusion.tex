\todo{Restate empirical contributions from earlier}

\section{TLS 1.3}

Point out that the ``informing future protocol design'' is effectively
``making TLS 1.3 better''. This is the ``improve the security of the Internet
in the future'' part.

Call out explicit TLS 1.3 design decisions based on results described in this
dissertation. Perhaps some of this should be interleaved into the earlier
sections?

%\section{Engineering Challenges}
%
%For not being a ``systems'' field, there is still an absurd amount of
%engineering that largely goes unacknowledged, in order to write good
%measurement papers. Discuss some of these examples.
%
%Is this a fundamental state of being of the methodology, or are we doing something wrong?

\section{Weaknesses from Export Cryptography}

In this section, we distill our experiences from measuring export
cryptography into concrete recommendations for technologists and
policymakers. We also examine open questions raised while investigating the
impact of export cryptography.

\paragraph{Limit Complexity.}
Each of FREAK, Logjam, and DROWN were fundamentally caused by the complexity
of supporting multiple paths from the initiation of a connection through the
completion of the handshake. Complexity in a protocol is not a result of
having many rules that define the protocol, but instead stems from having
many exceptions to the rules, or by having inconsistent rules. Protocol
designers should strive to simplify the handshake process in future
protocols, such that it can be more easily examined and implemented. This
also lowers the barrier to formal verification of the protocol, which
provides an extra level of reassurance that the protocol lacks fundamental
issues.

Cryptographic standards are often vague, or leave many knobs to be tuned by
the developer, for the sake of being performant and extensible. These options
are often confusing, and leave a large onus on the developer to understand
the cryptographic details of RSA or algebraic groups. This phenomenon is not
solely limited to cryptographic APIs, Georgiev et al.\ discovered that SSL
certificate validation was broken in a large number of non-browser TLS
applications due to developers misunderstanding and misusing library
calls~\cite{most-dangerous-code-2012}.

Writers of cryptographic standards should work to limit complexity, and
design protocols with the developer in mind. Protocols should be designed to
limit the number of ways the implementation can make a mistake. Mistakes
should be easy to identify, and result in a failure to complete a handshake
or communicate, rather than create subtle differences that fundamentally flaw
security, but are hard to detect. Furthermore, any cryptographic action
performed in a protocol should have enough contextual information that it
cannot be repurposed to be used in another protocol.

\paragraph{Weakened Cryptography.}
Every form of cryptography that was weakened to comply with the legacy export
regulations has now been exploited to attack modern cryptography. Both types
of weakened key exchange were exploited in 2015: export RSA public-key
cryptography was exploited by FREAK, and export Diffie-Hellman was exploited
by Logjam. The remaining form of weakened cryptography, export symmetric
ciphers, was exploited by DROWN. Any protocol implementing both export and
non-export cryptography has an imbalance: there must exist some mechanism to
select between these two types of cryptography, and this mechanism must
require the use of strong cryptography to prevent attackers from downgrading
the connection to use the weakened form of cryptography.

The security of TLS and export cryptography have been fundamentally linked.
Export cryptography is a unique constraint with a dangerous goal: weaken
cryptography, without weakening cryptography. Internet measurement techniques
show us that the export regulations weakened protocol design to the point
where the regulations are directly harmful to the security of the Internet
today. These empirical techniques show that these attacks are not
theoretical, leveraging protocols that have long-since disappeared, but
instead are a dark side of backwards compatibility, harming real users today.
Although the regulations went out of effect by 1999, the cryptography
remains. At their respective times of disclosure, 36.7\% of IPv4 HTTPS hosts
were vulnerable to FREAK, 4.9\% were vulnerable to Logjam, and 26\% were
vulnerable to DROWN. In all cases, empirical research enabled the full
understanding of the effects and impacts of these issues.

FREAK, Logjam, and DROWN provide comprehensive evidence that the legacy
regulations of the 1990s harmed the security of users on the Internet today.
The technical results show that purposefully weakened cryptogra- phy should
be avoided, as support legacy protocols is often maintained in order to
provide backwards compatibility. Beyond that, historical evidence suggests
that we simply do not know how to weaken some portion of cryptography, with
weakening all other cryptography. While export cryptography is not the same
as a backdoor, nor as a “secure golden key”, current empirical technological
evidence suggests that cryptography is fragile enough as is, and that any
form of deliberate change to enable more parties to have access to plaintext
ultimately weakens cryptography for everyone.

Segregating international and domestic cryptography does not make sense in
context of the Internet. Users are mobile, and traffic often travels through
multiple countries en route to any given website. With the advent of CDNs,
websites that may traditionally have been hosted in the United States are now
cached and accessible in a distributed fashion across the entire world.
Assigning different encryption to users based on nationality is not feasible
on an open Internet.

\section{Generalizing DROWN}

The DROWN attack further exploited export-grade cryptography with an
additional novel insight: Bleichenbacher oracles need not be present in the
target protocol under attack, so long as the key is shared between the two
protocols. Modern TLS servers were at risk if they shared a key with an
\ssltwo server. While DROWN was caused by export cryptography, and uses a
cryptographic vulnerability in one protocol to attack another protocol, the
cross-protocol methodology can be extended to other TLS attacks, and to
simple key compromise.

Reusing TLS keys across multiple protocols, such as HTTPS, SMTP, and IMAP,
leads to an increased attack surface. Beyond DROWN, the TLS protocol has a
fundamentally cross-protocol attack surface. X.509 certificates are not bound
to any particular protocol or port. A key compromised on a single protocol
can be utilized to attack other protocols. The compromise need not be due to
a TLS protocol vulnerability, and could simply be due to a remote code
execution exploit. 

Beyond key compromise, specific vulnerabilities in the TLS protocol and older
implementations can be utilized in a cross-protocol context to attack users
of a web service without explicitly compromising the private key. This is
best shown by the DROWN vulnerability, in which the mere existence of an
\ssltwo host that shared a key with a TLS host enabled decryption of
otherwise secure TLS connections using modern cryptography.

Even if distinct services, such as mail and web servers, use
different keys, so long as they share any name on the certificate, the
transport-layer security of all connections to that name are limited to the
security of the weakest TLS implementation or configuration. Even traditionally
web-based padding oracle attacks, such as POODLE, or the AES-NI padding-oracle
in OpenSSL, non-web servers can be exploited by active attackers targeting web
users. The attacker can rewrite the TCP connection to an alternative port, and
fill-in any pre-handshake protocol dialogue (e.g. by sending an EHLO or
STARTTLS command in SMTP).

Examing vulnerabilites in a cross-protocol context increases attack surface.
Using Internet-wide scanning to measure the prevalence of key and name reuse
between HTTPS and non-web TLS protocols on different ports would allow us to
empircally bound this risk. Wildcard certificates slightly complicate this
process; an exploitable wildcard certificate could be used to attack multiple
names.

\section{Empirical Pitfalls}

\todo{finish things out with a word of warning}

Security driven by data. Is any of this relevant to users who are just trying
to secure this own networks?

Extending measurement from aggregations and ecosystems to behavior of
individual hosts at global scale. Can we track individual hosts appearing and
disappearing, and map changes to configuration in real-time?

Should empiricism be part of more traditional cryptography education and research?

Some word of warning about how measurement doesn't solve all our problems,
and how measuring the wrong things makes things worse, \eg Robert McNamera's
use of body count as a metric during the Vietnam War.

% What the fuck does anything have to do with Vietnam?


