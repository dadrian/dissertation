This disseration shows that it is both possible, useful, and interesting to
study cryptography empirically. In this section, I draw from each of the case
studies presented, and discuss how cryptography should and has moved
forward. I also discuss how measuring cryptography fits into the larger theme
of data-driven, empirical security.

\section{TLS 1.3}

Empirical cryptography informed future protocol design in addition to
improving the current security of the Internet. TLS 1.3~\cite{rfc8446} was
standardized in August, 2018 and has been informed by the failings of TLS
1.2~\cite{tls-ietf-inria-bug,tls-ietf-smacktls,tls-ietf-rsa-pss} over the
past ten years. Many of the recommendations from prior chapters are followed
in TLS 1.3.

\paragraph{Named Groups.}
TLS 1.3 unifies parameters selection between elliptic curve and finite field
Diffie-Hellman. It removes support for custome \dhe{} groups, and instead
provides a method to select from a preset list of groups. Each of the
predefined groups has a minimum length of 2048 bits, and uses a safe
prime~\cite{rfc7919}.

\paragraph{Limiting RSA.}
RSA key exchange was removed entirely. For signatures, RSA \PKCS was removed
in favor of RSA PSS, which limits Bleichenbacher-style
attacks~\cite{exact-security-rsa}. Removing support for malleable \PKCS
signatures prevents DROWN-like oracles.

\paragraph{Explicit Verification.}
TLS 1.3 introduces the \textsf{CertificateVerify} message. Similar to key
exchange in SSH~\cite{rfc4253}, the \textsf{CertificateVerify} contains a
signature over the protocol transcript from the long-term private key of the
remote peer extracted from the X.509 certificate for that peer. This
explicitly binds the state of the connection to key. Had a similar mechanism
been present in TLS 1.2 and earlier, attacks such as FREAK and Logjam, where
the attacker breaks the session secret without compromising the long-term
secret, would not be possible.

\paragraph{New Dangers.}
DROWN oracles were only possible in \ssltwo because, unlike TLS, the server
sends secret-dervied data to the client, before verifying the client actually
poesses the secret. In TLS 1.2, the client sends its \textsf{Finished}
message first. This changes in TLS 1.3, where the server sends encrypted
\textsf{Certificate} messages prior to verifying the client poesses its key
exchange secret. Furthermore, TLS 1.3 introduces 0-RTT mode in which the
client sends application data in the first flight, and the server sends
application data before receiving the \textsf{ClientFinished} message. As
noted in the RFC, this is not forward secret and provides no anti-replay
guarantees~\cite{rfc8446}. While this clearly has application layer concerns,
if an application is vulnerable to replay attacks, it is also one of several
necessary conditions for DROWN-like attacks. This will likely be a source of
vulnerability in the future, however the extent of the vulnerability remains
to be seen.

\if0
Recommendations from earlier sections:

% logjam
- don't weaken crypto
- elliptic curves
- Avoid fixed-prime 1024-bit groups
- 2048-bit or longer DHE

% subgroup
- safe primes, 2048-bit, with exponents of at least 224 bits
- named curves

% drown
- rsa kex is bad
- reuse of server-side nonce by the client
- server does computation and sends response using secret without verifying client has the secret
- deterministic parameters from premaster secret and nonce
- remove deprecated technologies before they become vulnerabilites

% TLS1.3
- uses named diffie hellman groups with safe primes and minimum length 2048-bits (RFC 7919)
- only other thing to avoid small subgroup attacks is check 1 < Y < p - 1 and select a good exponent
- same DH as ECDH selection method
- no RSA key exchange
- certificate verify (poession of long term key)
- sends encrypted message with derived key before client (Bad!-ish!)

\fi

%\section{Engineering Challenges}
%
%For not being a ``systems'' field, there is still an absurd amount of
%engineering that largely goes unacknowledged, in order to write good
%measurement papers. Discuss some of these examples.
%
%Is this a fundamental state of being of the methodology, or are we doing something wrong?

\section{Weaknesses from Export Cryptography}

In this section, we distill our experiences from measuring export
cryptography into concrete recommendations for technologists and
policymakers. We also examine open questions raised while investigating the
impact of export cryptography.

\paragraph{Limit Complexity.}
Each of FREAK, Logjam, and DROWN were fundamentally caused by the complexity
of supporting multiple paths from the initiation of a connection through the
completion of the handshake. Complexity in a protocol is not a result of
having many rules that define the protocol, but instead stems from having
many exceptions to the rules, or by having inconsistent rules. Protocol
designers should strive to simplify the handshake process in future
protocols, such that it can be more easily examined and implemented. This
also lowers the barrier to formal verification of the protocol, which
provides an extra level of reassurance that the protocol lacks fundamental
issues.

Cryptographic standards are often vague, or leave many knobs to be tuned by
the developer, for the sake of being performant and extensible. These options
are often confusing, and leave a large onus on the developer to understand
the cryptographic details of RSA or algebraic groups. This phenomenon is not
solely limited to cryptographic APIs, Georgiev et al.\ discovered that SSL
certificate validation was broken in a large number of non-browser TLS
applications due to developers misunderstanding and misusing library
calls~\cite{most-dangerous-code-2012}.

Writers of cryptographic standards should work to limit complexity, and
design protocols with the developer in mind. Protocols should be designed to
limit the number of ways the implementation can make a mistake. Mistakes
should be easy to identify, and result in a failure to complete a handshake
or communicate, rather than create subtle differences that fundamentally flaw
security, but are hard to detect. Furthermore, any cryptographic action
performed in a protocol should have enough contextual information that it
cannot be repurposed to be used in another protocol.

\paragraph{Weakened Cryptography.}
Every form of cryptography that was weakened to comply with the legacy export
regulations has now been exploited to attack modern cryptography. Both types
of weakened key exchange were exploited in 2015: export RSA public-key
cryptography was exploited by FREAK, and export Diffie-Hellman was exploited
by Logjam. The remaining form of weakened cryptography, export symmetric
ciphers, was exploited by DROWN. Any protocol implementing both export and
non-export cryptography has an imbalance: there must exist some mechanism to
select between these two types of cryptography, and this mechanism must
require the use of strong cryptography to prevent attackers from downgrading
the connection to use the weakened form of cryptography.

The security of TLS and export cryptography have been fundamentally linked.
Export cryptography is a unique constraint with a dangerous goal: weaken
cryptography, without weakening cryptography. Internet measurement techniques
show us that the export regulations weakened protocol design to the point
where the regulations are directly harmful to the security of the Internet
today. These empirical techniques show that these attacks are not
theoretical, leveraging protocols that have long-since disappeared, but
instead are a dark side of backwards compatibility, harming real users today.
Although the regulations went out of effect by 1999, the cryptography
remains. At their respective times of disclosure, 36.7\% of IPv4 HTTPS hosts
were vulnerable to FREAK, 4.9\% were vulnerable to Logjam, and 26\% were
vulnerable to DROWN. In all cases, empirical research enabled the full
understanding of the effects and impacts of these issues.

FREAK, Logjam, and DROWN provide comprehensive evidence that the legacy
regulations of the 1990s harmed the security of users on the Internet today.
The technical results show that purposefully weakened cryptogra- phy should
be avoided, as support legacy protocols is often maintained in order to
provide backwards compatibility. Beyond that, historical evidence suggests
that we simply do not know how to weaken some portion of cryptography, with
weakening all other cryptography. While export cryptography is not the same
as a backdoor, nor as a “secure golden key”, current empirical technological
evidence suggests that cryptography is fragile enough as is, and that any
form of deliberate change to enable more parties to have access to plaintext
ultimately weakens cryptography for everyone.

Segregating international and domestic cryptography does not make sense in
context of the Internet. Users are mobile, and traffic often travels through
multiple countries en route to any given website. With the advent of CDNs,
websites that may traditionally have been hosted in the United States are now
cached and accessible in a distributed fashion across the entire world.
Assigning different encryption to users based on nationality is not feasible
on an open Internet.

\section{Generalizing DROWN}

The DROWN attack further exploited export-grade cryptography with an
additional novel insight: Bleichenbacher oracles need not be present in the
target protocol under attack, so long as the key is shared between the two
protocols. Modern TLS servers were at risk if they shared a key with an
\ssltwo server. While DROWN was caused by export cryptography, and uses a
cryptographic vulnerability in one protocol to attack another protocol, the
cross-protocol methodology can be extended to other TLS attacks, and to
simple key compromise.

Reusing TLS keys across multiple protocols, such as HTTPS, SMTP, and IMAP,
leads to an increased attack surface. Beyond DROWN, the TLS protocol has a
fundamentally cross-protocol attack surface. X.509 certificates are not bound
to any particular protocol or port. A key compromised on a single protocol
can be utilized to attack other protocols. The compromise need not be due to
a TLS protocol vulnerability, and could simply be due to a remote code
execution exploit.

Beyond key compromise, specific vulnerabilities in the TLS protocol and older
implementations can be utilized in a cross-protocol context to attack users
of a web service without explicitly compromising the private key. This is
best shown by the DROWN vulnerability, in which the mere existence of an
\ssltwo host that shared a key with a TLS host enabled decryption of
otherwise secure TLS connections using modern cryptography. Even if distinct
services, such as mail and web servers, use different keys, so long as they
share any name on the certificate, the transport-layer security against
active attackers for all connections to that name are limited to the weakest
TLS implementation or configuration for that name. Wildcard certificates
slightly complicate this process; an exploitable wildcard certificate could
be used to attack multiple names.

Traditionally web-based padding oracle attacks, such as
POODLE~\cite{poodle-2014}, or the AES-NI padding-oracle in
OpenSSL~\cite{cve-2016-2107}, non-web servers can be further exploited by
active attackers targeting web users. The attacker can rewrite the TCP
connection to an alternative port, and fill-in any pre-handshake protocol
dialogue (e.g. by sending an EHLO or STARTTLS command in SMTP). Future work
can use Internet-wide scanning to determine the prevalence of key and name
reuse between HTTPS and non-web TLS protocols. This would be an empirical
upper bound for the increase in attack surface from considering TLS exploits
in a cross-protocol context.

\section{Applicability of Empirical Methods}

Empirical methods provide a way to answer questions and work towards
solutions of problems that were previously left unsolvable under the guise of
``backwards compatability''. Beyond this disseration, measurements from
Chrome influenced the design of version negotiation for TLS
1.3~\cite{why-tls-13-browsers-cloudflare}, measurements of \starttls support
lead to new techniques for mail transport
security~\cite{google-email-transparency-report,mail-2015,rfc8461}, and
studying browser HTTPS errors led to increased proportions of successful
connections~\cite{wild-warnings-2017}.

Unfortunately, empirical cryptography, and more broadly, empirical security,
is often out of reach of non-researchers. Security analysts and system
administrators can leverage Internet-wide scanning data and certificate
transparency logs (the same data used for empirical cryptography) to identify
their own assets, track bad actors, and monitor for phishing sites, but
collecting this data is often out of reach, due to the quantity, velocity,
and cost of collecting the data. Access to a full Internet-wide perspective
if often limited to large organizations with large security teams.

As empirical techniques such as Internet-wide scanning and passive network
monitoring improve, can we extend measurement from aggregations and
ecosystems to understanding the behavior of individual hosts at global scale?
Can we track individual hosts appearing and disappearing, and map changes to
configuration in real-time?

As we approch being able to index devices in the same way we can index web
pages, the barrier to entry for utilizing a global perspective to empirically
study cryptography will decrease, and this decrease will come with improved
access to network security data. For cryptography, this means future
protocols can be designed to solve problems experienced by current protocols,
with a turnaround time of less than the decade it took to design TLS 1.3. For
security, it means an increase in the use of data to show that an
organizations security posture is improving or degrading, and the ability for
all defenders to have the same network perspective and data as a targeted
attacker. Fully understanding an organizations exposure requires strong
empirical techniques. An attacker only needs to leverage an Internet-wide
perspective once to find a vulnerable asset, but defenders need to constantly
understand their exposure. You cannot improve cryptography that you do not
know is in use. You cannot defend what you do not know you own.

Empirical techniques inform defenders about the existence and scale of
problems, but they are no substitute for preventing problems before they
happen in the first place. Any cryptographic parameter that can be measured
in a handshake is a parameter that could be wrong and a source of
vulnerability. Protocols such as Wireguard~\cite{wireguard-2017} have no
cryptographic configuration beyond the key in use. If a peer presents the
wrong key, the connection will fail to open. Beyond key generation, there is
no way to misconfigure the cryptographic security of a Wireguard host. This
suggests that cryptography that is easily examined via Internet-wide scanning
may be some of the most \textit{fragile} cryptography.
Ultimately, leveraging secure development practices and building
security-by-default into more systems will push the envelope towards a more
secure Internet. We should work towards improving security on all fronts, not
just in ways we know how to measure.

% Some word of warning about how measurement doesn't solve all our problems,
% and how measuring the wrong things makes things worse, \eg Robert McNamera's
% use of body count as a metric during the Vietnam War.

% What the fuck does anything have to do with Vietnam?


